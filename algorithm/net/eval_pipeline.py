# coding:utf-8
import json
from pathlib import Path
from xml.etree import ElementTree as ET

import numpy as np
import torch
from torch import cuda
from PIL import Image
from prettytable import PrettyTable
from utils.box_utils import jaccard_overlap_numpy, center_to_corner_numpy, rescale_bbox
from utils.augmentation_utils import ToTensor
from utils.log_utils import Logger

from .dataset import VOCDataset
from .yolo import Yolo


class EvalPipeline:
    """ æµ‹è¯•æ¨¡å‹æµæ°´çº¿ """

    def __init__(self, model_path: str, dataset: VOCDataset, image_size=416, anchors: list = None,
                 conf_thresh=0.05, overlap_thresh=0.5, save_dir='eval', use_07_metric=False, use_gpu=True):
        """
        Parameters
        ----------
        model_path: str
            æ¨¡å‹æ–‡ä»¶è·¯å¾„

        dataset: VOCDataset
            æ•°æ®é›†

        image_size: int
            å›¾åƒå°ºå¯¸

        anchors: list
            è¾“å…¥ç¥ç»ç½‘ç»œçš„å›¾åƒå¤§å°ä¸º 416 æ—¶å¯¹åº”çš„å…ˆéªŒæ¡†

        conf_thresh: float
            ç½®ä¿¡åº¦é˜ˆå€¼

        overlap_thresh: float
            IOU é˜ˆå€¼

        save_dir: str
            æµ‹è¯•ç»“æœå’Œé¢„æµ‹ç»“æœæ–‡ä»¶çš„ä¿å­˜ç›®å½•

        use_07_metric: bool
            æ˜¯å¦ä½¿ç”¨ VOC2007 çš„ AP è®¡ç®—æ–¹æ³•

        use_gpu: bool
            æ˜¯å¦ä½¿ç”¨ GPU
        """
        self.use_gpu = use_gpu
        self.dataset = dataset
        self.image_size = image_size
        self.conf_thresh = conf_thresh
        self.overlap_thresh = overlap_thresh
        self.use_07_metric = use_07_metric
        self.save_dir = Path(save_dir)
        self.logger = Logger("mAP")

        self.model_path = Path(model_path)
        self.device = 'cuda' if use_gpu and cuda.is_available() else 'cpu'
        self.model = Yolo(self.dataset.n_classes, image_size, anchors)
        self.model.detector.conf_thresh = conf_thresh
        self.model = self.model.to(self.device)
        self.model.load(model_path)
        self.model.eval()
        self.logger.info(f"âš—ï¸ è½½å…¥æ¨¡å‹ï¼š{model_path}")

    @torch.no_grad()
    def eval(self):
        """ æµ‹è¯•æ¨¡å‹ï¼Œè·å– mAP """
        self._predict()
        self._get_ground_truth()
        return self._get_mAP()

    def _predict(self):
        """ é¢„æµ‹æ¯ä¸€ç§ç±»å­˜åœ¨äºå“ªäº›å›¾ç‰‡ä¸­ """
        self.preds = {c: {} for c in self.dataset.classes}
        transformer = ToTensor(self.image_size)

        print('ğŸ›¸ æ­£åœ¨é¢„æµ‹ä¸­...')
        for i, (image_path, image_name) in enumerate(zip(self.dataset.image_paths, self.dataset.image_names)):
            print(f'\rå½“å‰è¿›åº¦ï¼š{i/len(self.dataset):.0%}', end='')

            # è¯»å…¥å›¾ç‰‡
            image = np.array(Image.open(image_path).convert('RGB'))
            h, w, _ = image.shape

            # é¢„æµ‹
            x = transformer.transform(image).to(self.device)
            out = self.model.predict(x)
            if not out:
                continue

            for c, pred in out[0].items():
                pred = pred.numpy()
                mask = pred[:, 0] > self.conf_thresh

                # å¦‚æœæ²¡æœ‰ä¸€ä¸ªè¾¹ç•Œæ¡†çš„ç½®ä¿¡åº¦å¤§äºé˜ˆå€¼å°±çº¸æ¡è·³è¿‡è¿™ä¸ªç±»
                if not mask.any():
                    continue

                # ç­›é€‰å‡ºæ»¡è¶³é˜ˆå€¼æ¡ä»¶çš„è¾¹ç•Œæ¡†
                conf = pred[:, 0][mask]  # type:np.ndarray
                bbox = rescale_bbox(pred[:, 1:][mask], self.image_size, h, w)
                bbox = center_to_corner_numpy(bbox)

                # ä¿å­˜é¢„æµ‹ç»“æœ
                self.preds[self.dataset.classes[c]][image_name] = {
                    "bbox": bbox.tolist(),
                    "conf": conf.tolist()
                }

    def _get_ground_truth(self):
        """ è·å– ground truth ä¸­æ¯ä¸€ç§ç±»å­˜åœ¨äºå“ªäº›å›¾ç‰‡ä¸­ """
        self.ground_truths = {c: {} for c in self.dataset.classes}
        self.n_positives = {c: 0 for c in self.dataset.classes}

        print('\n\nğŸ§© æ­£åœ¨è·å–æ ‡ç­¾ä¸­...')
        for i, (anno_path, img_name) in enumerate(zip(self.dataset.annotation_paths, self.dataset.image_names)):
            print(f'\rå½“å‰è¿›åº¦ï¼š{i/len(self.dataset):.0%}', end='')

            root = ET.parse(anno_path).getroot()

            for obj in root.iter('object'):
                # è·å–æ ‡ç­¾å«æœ‰çš„çš„ç±»å’Œè¾¹ç•Œæ¡†
                c = obj.find('name').text.lower().strip()
                difficult = int(obj.find('difficult').text)
                bbox = obj.find('bndbox')
                bbox = [
                    int(bbox.find('xmin').text),
                    int(bbox.find('ymin').text),
                    int(bbox.find('xmax').text),
                    int(bbox.find('ymax').text),
                ]

                if not self.ground_truths[c].get(img_name):
                    self.ground_truths[c][img_name] = {
                        "bbox": [],
                        "detected": [],
                        "difficult": []
                    }

                # æ·»åŠ ä¸€æ¡ ground truth è®°å½•
                self.ground_truths[c][img_name]['bbox'].append(bbox)
                self.ground_truths[c][img_name]['detected'].append(False)
                self.ground_truths[c][img_name]['difficult'].append(difficult)
                self.n_positives[c] += (1-difficult)

    def _get_mAP(self):
        """ è®¡ç®— mAP """
        result = {}

        print('\n\nğŸ§ª æ­£åœ¨è®¡ç®— AP ä¸­...')
        mAP = 0
        table = PrettyTable(["class", "AP"])
        for c in self.dataset.classes:
            ap, precision, recall = self._get_AP(c)
            result[c] = {
                'AP': ap,
                'precision': precision,
                'recall': recall
            }
            mAP += ap
            table.add_row([c, f"{ap:.2%}"])

        mAP /= len(self.dataset.classes)
        table.add_column("mAP", [f"{mAP:.2%}"] + [""]
                         * (len(self.dataset.classes)-1))
        self.logger.info("mAP è®¡ç®—ç»“æœå¦‚ä¸‹ï¼š\n"+str(table))

        # ä¿å­˜è¯„ä¼°ç»“æœ
        self.save_dir.mkdir(exist_ok=True, parents=True)
        p = self.save_dir / (self.model_path.stem + '_AP.json')
        with open(p, 'w', encoding='utf-8') as f:
            json.dump(result, f)

        return mAP

    def _get_AP(self, c: str):
        """ è®¡ç®—ä¸€ä¸ªç±»çš„ AP

        Parameters
        ----------
        c: str
            ç±»åˆ«å

        Returns
        -------
        ap: float
            APï¼Œæ²¡æœ‰é¢„æµ‹å‡ºè¿™ä¸ªç±»å°±è¿”å› 0

        precision: list
            æŸ¥å‡†ç‡

        recall: list
            æŸ¥å…¨ç‡
        """
        pred = self.preds[c]
        ground_truth = self.ground_truths[c]
        bbox = []
        conf = []
        image_names = []

        # å°† bbox æ‹¼æ¥ä¸ºäºŒç»´çŸ©é˜µï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªé¢„æµ‹æ¡†
        for image_name, v in pred.items():
            image_names.extend([image_name]*len(v['conf']))
            bbox.append(v['bbox'])
            conf.append(v['conf'])

        # æ²¡æœ‰åœ¨ä»»ä½•ä¸€å¼ å›¾ç‰‡ä¸­é¢„æµ‹å‡ºè¿™ä¸ªç±»
        if not bbox:
            return 0, 0, 0

        bbox = np.vstack(bbox)  # type:np.ndarray
        conf = np.hstack(conf)  # type:np.ndarray
        image_names = np.array(image_names)

        # æ ¹æ®ç½®ä¿¡åº¦é™åºæ’åºé¢„æµ‹æ¡†
        index = np.argsort(-conf)
        bbox = bbox[index]
        conf = conf[index]
        image_names = image_names[index]

        # è®¡ç®— TP å’Œ FP
        tp = np.zeros(len(image_names))  # type:np.ndarray
        fp = np.zeros(len(image_names))  # type:np.ndarray
        for i, image_name in enumerate(image_names):
            # è·å–ä¸€å¼ å›¾ç‰‡ä¸­å…³äºè¿™ä¸ªç±»çš„ ground truth
            record = ground_truth.get(image_name)

            # è¿™å¼ å›¾ç‰‡çš„ ground_truth ä¸­æ²¡æœ‰è¿™ä¸ªç±»å°±å°† fp+1
            if not record:
                fp[i] = 1
                continue

            bbox_pred = bbox[i]  # shape:(4, )
            bbox_gt = np.array(record['bbox'])  # shape:(n, 4)
            difficult = np.array(record['difficult'], np.bool)  # shape:(n, )

            # è®¡ç®—äº¤å¹¶æ¯”
            iou = jaccard_overlap_numpy(bbox_pred, bbox_gt)
            iou_max = iou.max()
            iou_max_index = iou.argmax()

            if iou_max < self.overlap_thresh:
                fp[i] = 1
            elif not record['difficult'][iou_max_index]:
                # å·²ç»åŒ¹é…äº†é¢„æµ‹æ¡†çš„è¾¹ç•Œæ¡†ä¸èƒ½å†åŒ¹é…é¢„æµ‹æ¡†
                if not record['detected'][iou_max_index]:
                    tp[i] = 1
                    record['detected'][iou_max_index] = True
                else:
                    fp[i] = 1

        # æŸ¥å…¨ç‡å’ŒæŸ¥å‡†ç‡
        tp = tp.cumsum()
        fp = fp.cumsum()
        n_positives = self.n_positives[c]
        recall = tp / n_positives  # type:np.ndarray
        precision = tp / (tp + fp)  # type:np.ndarray

        # è®¡ç®— AP
        if not self.use_07_metric:
            rec = np.concatenate(([0.], recall, [1.]))
            prec = np.concatenate(([0.], precision, [0.]))

            # è®¡ç®— PR æ›²çº¿çš„åŒ…ç»œçº¿
            for i in range(prec.size-1, 0, -1):
                prec[i - 1] = np.maximum(prec[i - 1], prec[i])

            # æ‰¾å‡º recall å˜åŒ–æ—¶çš„ç´¢å¼•
            i = np.where(rec[1:] != rec[:-1])[0]

            # ç”¨recallçš„é—´éš”å¯¹ç²¾åº¦ä½œåŠ æƒå¹³å‡
            ap = np.sum((rec[i + 1] - rec[i]) * prec[i + 1])
        else:
            ap = 0
            for r in np.arange(0, 1.1, 0.1):
                if np.any(recall >= r):
                    ap += np.max(precision[recall >= r])/11

        return ap, precision.tolist(), recall.tolist()
